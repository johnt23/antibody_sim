# (a) Solution:  Compute bias for both estimators
bias1 = mean(est.vec.1-exp.lambda)
bias2 = mean(est.vec.2-exp.lambda)
bias1.mc.error = sd(est.vec.1-exp.lambda)/sqrt(sim.size)
bias2.mc.error = sd(est.vec.2-exp.lambda)/sqrt(sim.size)
bias1.mc.95CI = bias1 + c(-1.96,1.96)*bias1.mc.error
bias2.mc.95CI = bias2 + c(-1.96,1.96)*bias2.mc.error
var1 = mean((est.vec.1-lambda)^2)
var1 = mean((est.vec.1-exp.lambda)^2)
var2 = mean((est.vec.2-exp.lambda)^2)
mean(n/(est.vec.1)^2)
mean(sample.size/(est.vec.1)^2)
mean(est.vec.2)
3.1/34
boxplot(list(est1=est.vec.1,est2=est.vec.2))
set.seed(3805830)
# Run MC sims for each estimator
sim.size = 100000
sample.size = 35
exp.lambda = 3.1
exp.data = matrix(rexp(n=sim.size*sample.size,
rate=exp.lambda),nrow=sim.size,
ncol=sample.size)
exp.est.1 = function(exp.samp){return(1/mean(exp.samp))}
exp.est.2 = function(exp.samp){return(1/sd(exp.samp))}
est.vec.1 = numeric(sim.size)
est.vec.2 = numeric(sim.size)
for (i in 1:sim.size){
est.vec.1[i] = exp.est.1(exp.data[i,])
est.vec.2[i] = exp.est.2(exp.data[i,])
}
# (a) Solution:  Compute bias for both estimators
bias1 = mean(est.vec.1-exp.lambda)
bias2 = mean(est.vec.2-exp.lambda)
bias1.mc.error = sd(est.vec.1-exp.lambda)/sqrt(sim.size)
bias2.mc.error = sd(est.vec.2-exp.lambda)/sqrt(sim.size)
bias1.mc.95CI = bias1 + c(-1.96,1.96)*bias1.mc.error
bias2.mc.95CI = bias2 + c(-1.96,1.96)*bias2.mc.error
boxplot(list(est1=est.vec.1,est2=est.vec.2))
mean((est.vec.1)^2/samp.size)
mean((est.vec.1)^2/sample.size)
mean((est.vec.2)^2/sample.size)
hist(est.vec.1)
hist(est.vec.2)
var1 = mean(sample.size/(est.vec.1)^2)
var2 = mean(sample.size/(est.vec.2)^2)
sqrt(var1)
sqrt(var2)
boxplot(list(est1=est.vec.1,est2=est.vec.2))
mse1 = mean((est.vec.1-exp.lambda)^2)
mse2 = mean((est.vec.2-exp.lambda)^2)
mse1.mc.error = sd((est.vec.1-exp.lambda)^2)/sqrt(sim.size)
mse2.mc.error = sd((est.vec.2-exp.lambda)^2)/sqrt(sim.size)
mse1.mc.95CI = mse1 + c(-1.96,1.96)*mse1.mc.error
mse2.mc.95CI = mse2 + c(-1.96,1.96)*mse2.mc.error
mean((est.vec.1)^2/sample.size)
bias1^2
mean((est.vec.2)^2/sample.size)
bias2^2
mean(1/est.vec.1^2)
mean(1/est.vec.2^2)
set.seed(3805830)
# Run MC sims for each estimator
sim.size = 100000
sample.size = 35
exp.lambda = 3.1
exp.data = matrix(rexp(n=sim.size*sample.size,
rate=exp.lambda),nrow=sim.size,
ncol=sample.size)
exp.est.1 = function(exp.samp){return(1/mean(exp.samp))}
exp.est.2 = function(exp.samp){return(1/sd(exp.samp))}
est.vec.1 = numeric(sim.size)
est.vec.2 = numeric(sim.size)
for (i in 1:sim.size){
est.vec.1[i] = exp.est.1(exp.data[i,])
est.vec.2[i] = exp.est.2(exp.data[i,])
}
# (a) Solution:  Compute bias for both estimators
bias1 = mean(est.vec.1-exp.lambda)
bias2 = mean(est.vec.2-exp.lambda)
bias1.mc.error = sd(est.vec.1-exp.lambda)/sqrt(sim.size)
bias2.mc.error = sd(est.vec.2-exp.lambda)/sqrt(sim.size)
bias1.mc.95CI = bias1 + c(-1.96,1.96)*bias1.mc.error
bias2.mc.95CI = bias2 + c(-1.96,1.96)*bias2.mc.error
# boxplot(list(est1=est.vec.1,est2=est.vec.2))
# (b) Solution:  Compute variance for both estimators
# For the exponential, Var(X) ~ lambda_hat^2/n
var1 = mean((est.vec.1-exp.lambda)^2/sample.size)
var2 = mean((est.vec.2-exp.lambda)^2/sample.size)
boxplot(list(est1=est.vec.1,est2=est.vec.2))
sqrt(0.3)
sqrt(0.34)
hist(est.vec.1)
curve(dnorm(x, mean=3.1, sd=0.58309),
col="darkblue", lwd=2, add=TRUE, yaxt="n")
hist(est.vec.2,density)
dnorm(x, mean=3.1, sd=0.58309)
sd(est.vec.2)
var(est.vec.2)
# (c) Solution:
mse1 = mean((est.vec.1-exp.lambda)^2)
mse2 = mean((est.vec.2-exp.lambda)^2)
mse1.mc.error = sd((est.vec.1-exp.lambda)^2)/sqrt(sim.size)
mse2.mc.error = sd((est.vec.2-exp.lambda)^2)/sqrt(sim.size)
mse1.mc.95CI = mse1 + c(-1.96,1.96)*mse1.mc.error
mse2.mc.95CI = mse2 + c(-1.96,1.96)*mse2.mc.error
bias2^2+var(est.vec.2)
bias1^2+var(est.vec.1)
sd(var(1,2,3))
sqrt(var1)
set.seed(3805830)
# Run MC sims for each estimator
sim.size = 100000
sample.size = 35
exp.lambda = 3.1
exp.data = matrix(rexp(n=sim.size*sample.size,
rate=exp.lambda),nrow=sim.size,
ncol=sample.size)
exp.est.1 = function(exp.samp){return(1/mean(exp.samp))}
exp.est.2 = function(exp.samp){return(1/sd(exp.samp))}
est.vec.1 = numeric(sim.size)
est.vec.2 = numeric(sim.size)
for (i in 1:sim.size){
est.vec.1[i] = exp.est.1(exp.data[i,])
est.vec.2[i] = exp.est.2(exp.data[i,])
}
# (a) Solution:  Compute bias for both estimators
bias1 = mean(est.vec.1-exp.lambda)
bias2 = mean(est.vec.2-exp.lambda)
bias1.mc.error = sd(est.vec.1-exp.lambda)/sqrt(sim.size)
bias2.mc.error = sd(est.vec.2-exp.lambda)/sqrt(sim.size)
bias1.mc.95CI = bias1 + c(-1.96,1.96)*bias1.mc.error
bias2.mc.95CI = bias2 + c(-1.96,1.96)*bias2.mc.error
# boxplot(list(est1=est.vec.1,est2=est.vec.2))
# (b) Solution:  Compute variance for both estimators
var1 = var(est.vec.1)
var2 = var(est.vec.2)
var1.mc.error = sqrt(var1)
var2.mc.error = sqrt(var2)
var1.mc.95CI = var1 + c(-1.96,1.96)*var1.mc.error
var2.mc.95CI = var2 + c(-1.96,1.96)*var2.mc.error
# (c) Solution:
mse1 = mean((est.vec.1-exp.lambda)^2)
mse2 = mean((est.vec.2-exp.lambda)^2)
mse1.mc.error = sd((est.vec.1-exp.lambda)^2)/sqrt(sim.size)
mse2.mc.error = sd((est.vec.2-exp.lambda)^2)/sqrt(sim.size)
mse1.mc.95CI = mse1 + c(-1.96,1.96)*mse1.mc.error
mse2.mc.95CI = mse2 + c(-1.96,1.96)*mse2.mc.error
sqrt(var1/sim.size)
sqrt(var2/sim.size)
var1 + c(-1.96,1.96)*var1.mc.error
var1.mc.error = sqrt(var1/sim.size)
var2.mc.error = sqrt(var2/sim.size)
var1.mc.95CI = var1 + c(-1.96,1.96)*var1.mc.error
var2.mc.95CI = var2 + c(-1.96,1.96)*var2.mc.error
6690/557.5
7200/600
sim.size = 100000
sample.size = 100
pois.lambda = 4
rm(list=ls())
set.seed(378923)
samp.size <- 21
alpha <- .01
p <- 0.5
sim.size <- 10000           # number of replicates
p.vec <- numeric(sim.size)      # storage for p-values
p.vec.2 <- numeric(sim.size)
for (sim in 1:sim.size) {
samp <- rbinom(samp.size,1,prob=p) # simulate from H0
K <- sum(samp)
p.vec[sim] <- binom.test(x=K,n=samp.size,p=p,alternative = "greater")$p.value
p.vec.2[sim] <- pbinom((K-1), size=samp.size, prob=p, lower.tail=FALSE)
#ttest <- t.test(samp, alternative = "greater", mu = p)
#p.vec[sim] <- ttest$p.value
}
(p.hat <- mean(p.vec < alpha))
p.hat <- mean(p.vec.2 < alpha)
(p.hat <- mean(p.vec < alpha))
(p.hat <- mean(p.vec.2 < alpha))
rm(list=ls())
set.seed(378923)
samp.size <- 21
alpha <- .01
p <- 0.5
sim.size <- 10000           # number of replicates
p.vec <- numeric(sim.size)      # storage for p-values
p.vec.2 <- numeric(sim.size)
for (sim in 1:sim.size) {
samp <- rbinom(samp.size,1,prob=p) # simulate from H0
K <- sum(samp)
p.vec[sim] <- binom.test(x=K,n=samp.size,p=p,alternative = "greater")$p.value
p.vec.2[sim] <- pbinom((K-1), size=samp.size, prob=p, lower.tail=FALSE)
#ttest <- t.test(samp, alternative = "greater", mu = p)
#p.vec[sim] <- ttest$p.value
}
(p.hat <- mean(p.vec < alpha))
(p.hat <- mean(p.vec.2 < alpha))
(se.hat <- sqrt(p.hat*(1-p.hat)/sim.size))
p.hat + 1.96 * c(-1,1) * se.hat
set.seed(378923)
sim.size = 10000
true.alpha = 0.01
p.null = 0.5
sample.size = 21
test.pvalue = function(K.test,n, p0=0.5){
##Pr(K>=K.test|p=p0)=Pr(K>(K.test-1)|p=p0)
return(pbinom((K.test-1), size=n, prob=p0, lower.tail=FALSE))
}
##Type I error
p.values =numeric(sim.size)
for( i in 1:sim.size){
bernoulli.sample = rbinom(sample.size,p=p.null,size=1)
K.test = sum(bernoulli.sample)
p.values[i] = test.pvalue(K.test,sample.size,p.null )
}
##MC est of Type I error
(MC.alpha = mean( p.values < true.alpha))
(MC.alpha.error = sqrt( MC.alpha*(1-MC.alpha)/sim.size ))
(MC.alpha.ci = MC.alpha+c(-1,1)*1.96*MC.alpha.error)
rm(list=ls())
set.seed(378923)
samp.size <- 21
alpha <- .01
p <- 0.5
sim.size <- 10000           # number of replicates
p.vec <- numeric(sim.size)      # storage for p-values
p.vec.2 <- numeric(sim.size)
for (sim in 1:sim.size) {
samp <- rbinom(samp.size,1,prob=p) # simulate from H0
K <- sum(samp)
p.vec[sim] <- binom.test(x=K,n=samp.size,p=p,alternative = "greater")$p.value
p.vec.2[sim] <- pbinom((K-1), size=samp.size, prob=p, lower.tail=FALSE)
#ttest <- t.test(samp, alternative = "greater", mu = p)
#p.vec[sim] <- ttest$p.value
}
(p.hat <- mean(p.vec < alpha))
(p.hat <- mean(p.vec.2 < alpha))
(se.hat <- sqrt(p.hat*(1-p.hat)/sim.size))
p.hat + 1.96 * c(-1,1) * se.hat
p.alt <- 0.7
samp.alt <- seq(20,50,by=5)
n.alts <- length(samp.alt)
power <- numeric(n.alts)
for (i in 1:n.alts) {
n.a <- samp.alt[i] # Select an alternative
p.val <- numeric(sim.size)
for(sim in 1:sim.size) {
# simulated from alternative
samp <- rbinom(n.a,1,prob=p.alt) # simulate from H0
# Perform binomial test
K <- sum(samp)
p.val[sim] <- binom.test(x=K,n=n.a,p=p,alternative = "greater")$p.value
}
power[i] <- mean(p.val < alpha)
# We are counting the rejections: small p-values.
}
se <- sqrt(power * (1-power) / sim.size)
plot(samp.alt, power, cex=.75, pch=16, col='red', xlab='p')
abline(h = .01, lty = 1)
text(40, .01, expression(alpha), pos=3)
lines(samp.alt, power, lty=3)
# Error bars-- very small
arrows(c(samp.alt, samp.alt), c(power, power), c(samp.alt, samp.alt),
c(power+1.96*se, power-1.96*se), length=.05, angle=90)
power
set.seed(378923)
sim.size = 10000
true.alpha = 0.01
p.null = 0.5
sample.size = 21
test.pvalue = function(K.test,n, p0=0.5){
##Pr(K>=K.test|p=p0)=Pr(K>(K.test-1)|p=p0)
return(pbinom((K.test-1), size=n, prob=p0, lower.tail=FALSE))
}
##Type I error
p.values =numeric(sim.size)
for( i in 1:sim.size){
bernoulli.sample = rbinom(sample.size,p=p.null,size=1)
K.test = sum(bernoulli.sample)
p.values[i] = test.pvalue(K.test,sample.size,p.null )
}
##MC est of Type I error
(MC.alpha = mean( p.values < true.alpha))
(MC.alpha.error = sqrt( MC.alpha*(1-MC.alpha)/sim.size ))
(MC.alpha.ci = MC.alpha+c(-1,1)*1.96*MC.alpha.error)
p_a = 0.7
n_a = c(20, 25, 30, 35, 40, 45, 50)
power = numeric(length(n_a))
for ( k in 1:length(n_a)){
p.values = numeric(sim.size)
for( i in 1:sim.size){
bernoulli.sample = rbinom(n_a[k],p=p_a,size=1)
K.test = sum(bernoulli.sample)
p.values[i] = test.pvalue(K.test,n_a[k],p.null)
}
power[k] = mean(p.values<true.alpha)
}
power.se = sqrt(power*(1-power)/sim.size)
plot(n_a, power, cex=.75, pch=16, col='red',
xlab="n")
lines(n_a, power, lty=3)
arrows(c(n_a, n_a), c(power, power), c(n_a, n_a),
c(power+1.96*power.se, power-1.96*power.se), length=.05, angle=90)
power
help(rfsrc)
help('rfsrc')
?rfsrc
??rfsrc
# herring_summary_stats.r
# Created by John Trochta
# Summary:
#   Runs series of summary statistics on time series for figures & numbers
#   presented in manuscript.
#   Compiles bits of code from various other R files that runs calculations.
library(dplyr)
library(RcppRoll)
library(gridExtra)
library(cowplot)
library(pracma)
library(ggExtra)
library(reshape2)
library(data.table)
library(gtable)
#narrowed.down <- read.csv(here::here("data/my_data/compiled_herring_data_December2018.csv"),header=TRUE,stringsAsFactors = FALSE,row.names=1)
output.location <- here::here("results/2019_12/")
narrowed.down <- read.csv(paste0(output.location,"/processed_herring_data.csv"),header=TRUE,stringsAsFactors = FALSE,row.names=1)
source(here::here("src/function_process_data.R"))
source(here::here("src/function_calc_beta.R"))
source(here::here("src/function_calc_hurst.R"))
dat_stat <- function(her_dat){
lag.step <- 1
method <- "spearman"
her_dat_stat <- her_dat %>%
filter(!(secondinfo %in% c('Yellow Sea','Area 27 (North Vancouver Island)')) & !is.na(rel.bio)) %>%
group_by(StockID,species,mainname,secondinfo,biomass.type,estimate.method,recruit.type) %>%
filter(length(rel.bio)>=10) %>%
do(data.frame(TS.LENGTH = length(.$rel.bio),
TS.MEDIAN = median(.$rel.bio,na.rm=TRUE),
# Biomass stats
CV.BIO = sd(.$rel.bio,na.rm=TRUE)/mean(.$rel.bio,na.rm=TRUE),
AC.BIO = acf(x=.$rel.bio,lag.max=lag.step,plot=FALSE)$acf[lag.step+1,1,1],
HURST.BIO = function_calc_hurst(.$rel.bio), # 0.5-1 is persistence, 0.5 is uncorrelated/random walk
SPEC.BIO = ifelse(length(.$rel.bio)<8,NA,function_calc_beta(.$rel.bio)[1]),
BELOW.15 = any(.$rel.bio<0.15),
BELOW.3 = any(.$rel.bio<0.3),
SEC.MAX.1YR.DECLINE = min((.$rel.bio-lag(.$rel.bio,n=1))[min_rank(.$rel.bio-lag(.$rel.bio,n=1))==2],na.rm=TRUE),
MAX.1YR.DECLINE = min(.$rel.bio-lag(.$rel.bio,n=1),na.rm=TRUE),
MAX.1YR.INCREASE = max(.$rel.bio-lag(.$rel.bio,n=1),na.rm=TRUE),
MEDIAN.1YR.CHANGE = median(.$rel.bio-lag(.$rel.bio,n=1),na.rm=TRUE),
MEAN.BOTTOM.SSB= (mean(.$rel.bio[.$rel.bio<quantile(.$rel.bio,probs=0.1,na.rm=TRUE)],na.rm=TRUE)),
MIN.BIOMASS=min(.$rel.bio,na.rm=TRUE),
MEAN.LOW.BIOMASS=mean(.$rel.bio[min_rank(.$rel.bio)%in%(1:3)],na.rm=TRUE),
MAX.BIOMASS=max(.$rel.bio,na.rm=TRUE),
HIGHEST.AFTER.MIN=max(.$rel.bio[which(.$rel.bio==min(.$rel.bio,na.rm=TRUE))[1]:length(.$rel.bio)],na.rm=TRUE),
WHEN.HIGHEST.AFTER.MIN=which(.$rel.bio==max(.$rel.bio[which(.$rel.bio==min(.$rel.bio,na.rm=TRUE))[1]:length(.$rel.bio)],na.rm=TRUE))[1]-
which(.$rel.bio==min(.$rel.bio,na.rm=TRUE))[1],
YEAR.MAX=.$year[.$rel.bio==max(.$rel.bio,na.rm=TRUE)][1],
YEAR.MIN=.$year[.$rel.bio==min(.$rel.bio,na.rm=TRUE)][1],
RECENT.AVG.BIO=median(.$rel.bio[.$year>2000],na.rm=TRUE),
AVG.BIO.90s=median(.$rel.bio[.$year>1990 & .$year<=2000],na.rm=TRUE),
AVG.BIO.80s=median(.$rel.bio[.$year>1980 & .$year<=1990],na.rm=TRUE),
AVG.BIO.70s=median(.$rel.bio[.$year>1970 & .$year<=1980],na.rm=TRUE),
AVG.BIO.60s=median(.$rel.bio[.$year>1960 & .$year<=1970],na.rm=TRUE),
# Recruit stats
AGE.AT.REC = unique(.$recruitment.age),
AGE.AT.MAT = unique(.$age.at.maturity),
MIN.REC=min(.$rel.rec,na.rm=TRUE),
MAX.REC=max(.$rel.rec,na.rm=TRUE),
TS.LENGTH.REC = sum(!is.na(.$rel.rec)),
CV.REC = sd(.$recruits,na.rm=TRUE)/mean(.$recruits,na.rm=TRUE),
AC.REC = ifelse(length(.$recruits[!is.na(.$recruits)])<=10,NA,acf(x=.$recruits[!is.na(.$recruits)],lag.max=lag.step,plot=FALSE)$acf[lag.step+1,1,1]),
HURST.REC = ifelse(length(.$recruits[!is.na(.$recruits)])<=10,NA,function_calc_hurst(.$recruits[!is.na(.$recruits)])), # 0.5-1 is persistence, 0.5 is uncorrelated/random walk
SPEC.REC = ifelse(length(.$recruits[!is.na(.$recruits)])<=10,NA,function_calc_beta(.$recruits[!is.na(.$recruits)])[1]), # 0 is white noise, near 1 is red, near 2 is intense brown
CV.LOG.REC = sd(.$log.recruits,na.rm=TRUE)/mean(.$log.recruits,na.rm=TRUE),
AC.LOG.REC = ifelse(length(.$recruits[!is.na(.$recruits)])<=10,NA,acf(x=log(.$recruits[!is.na(.$recruits)]),lag.max=lag.step,plot=FALSE)$acf[lag.step+1,1,1]),
HURST.LOG.REC = ifelse(length(.$log.recruits[!is.na(.$log.recruits)])<=10,NA,function_calc_hurst(.$log.recruits[!is.na(.$log.recruits)])), # 0.5-1 is persistence, 0.5 is uncorrelated/random walk
SPEC.LOG.REC = ifelse(length(.$log.recruits[!is.na(.$log.recruits)])<=10,NA,function_calc_beta(.$log.recruits[!is.na(.$log.recruits)])[1]), # 0 is white noise, near 1 is red, near 2 is intense brown
AC.STD.REC = ifelse(length(.$recruits[!is.na(.$recruits)])<=10,NA,acf(x=.$std.rec[!is.na(.$std.rec)],lag.max=lag.step,plot=FALSE)$acf[lag.step+1,1,1]),
HURST.STD.REC = ifelse(length(.$recruits[!is.na(.$recruits)])<=10,NA,function_calc_hurst(.$std.rec[!is.na(.$std.rec)])), # 0.5-1 is persistence, 0.5 is uncorrelated/random walk
SPEC.STD.REC = ifelse(length(.$recruits[!is.na(.$recruits)])<=10,NA,function_calc_beta(.$std.rec[!is.na(.$std.rec)])[1]), # 0 is white noise, near 1 is red, near 2 is intense brown
AC.STD.LOG.REC = ifelse(length(.$std.log.rec[!is.na(.$std.log.rec)])<=10,NA,acf(x=.$std.log.rec[!is.na(.$std.log.rec)],lag.max=lag.step,plot=FALSE)$acf[lag.step+1,1,1]),
HURST.STD.LOG.REC = ifelse(length(.$std.log.rec[!is.na(.$std.log.rec)])<=10,NA,function_calc_hurst(.$std.log.rec[!is.na(.$std.log.rec)])), # 0.5-1 is persistence, 0.5 is uncorrelated/random walk
SPEC.STD.LOG.REC = ifelse(length(.$std.log.rec[!is.na(.$std.log.rec)])<=10,NA,function_calc_beta(.$std.log.rec[!is.na(.$std.log.rec)])[1]), # 0 is white noise, near 1 is red, near 2 is intense brown
#MAX.1YR.DECLINE.REC = min(.$rel.rec-lag(.$rel.rec,n=1),na.rm=TRUE),
MAX.1YR.INCREASE.REC = max(.$rel.rec-lag(.$rel.rec,n=1),na.rm=TRUE),
SEC.MAX.REC = max((.$rel.rec-lag(.$rel.rec,n=1))[rank(.$rel.rec/lag(.$rel.rec,n=1))==2],na.rm=TRUE),
NO.STRONG.COHORTS = sum((.$rel.rec-lag(.$rel.rec,n=1))>=1,na.rm=TRUE),
WHEN.STRONG.COHORTS = ifelse(all(.$rel.rec-lag(.$rel.rec,n=1)<1),NA,paste0(.$year[(.$rel.rec-lag(.$rel.rec,n=1))>=1 & !is.na(.$rel.rec)],collapse=",")),
MEDIAN.1YR.CHANGE.REC = median(.$rel.rec-lag(.$rel.rec,n=1),na.rm=TRUE),
#MEAN.REC.DURING.MIN.BIO =  median(.$rel.rec[(which(.$rel.bio==min(.$rel.bio,na.rm=TRUE))[1]-2):(which(.$rel.bio==min(.$rel.bio,na.rm=TRUE))[1]+2)],na.rm=TRUE),
MEDIAN.REC=median(.$rel.rec,na.rm=TRUE),
REC.ABOVE.5 = sum(.$rel.rec>0.5,na.rm=TRUE)/sum(!is.na(.$rel.rec)),
REC.BELOW.15 = sum(.$rel.rec<0.15,na.rm=TRUE)/sum(!is.na(.$rel.rec)),
REC.BELOW.3 = sum(.$rel.rec<0.3,na.rm=TRUE)/sum(!is.na(.$rel.rec)),
MEAN.BOTTOM.REC= (mean(.$rel.rec[.$rel.bio<quantile(.$rel.bio,probs=0.1,na.rm=TRUE)],na.rm=TRUE)),
MEAN.BOTTOM.PRO= (mean(.$productivity[.$rel.bio<quantile(.$rel.bio,probs=0.1,na.rm=TRUE)],na.rm=TRUE)),
RECENT.AVG.REC=median(.$rel.rec[.$year>2000],na.rm=TRUE),
AVG.REC.90s=median(.$rel.rec[.$year>1990 & .$year<=2000],na.rm=TRUE),
AVG.REC.80s=median(.$rel.rec[.$year>1980 & .$year<=1990],na.rm=TRUE),
AVG.REC.70s=median(.$rel.rec[.$year>1970 & .$year<=1980],na.rm=TRUE),
AVG.REC.60s=median(.$rel.rec[.$year>1960 & .$year<=1970],na.rm=TRUE),
RECENT.AVG.CAT=median(.$rel.cat[.$year>2000],na.rm=TRUE),
AVG.CAT.90s=median(.$rel.cat[.$year>1990 & .$year<=2000],na.rm=TRUE),
AVG.CAT.80s=median(.$rel.cat[.$year>1980 & .$year<=1990],na.rm=TRUE),
AVG.CAT.70s=median(.$rel.cat[.$year>1970 & .$year<=1980],na.rm=TRUE),
AVG.CAT.60s=median(.$rel.cat[.$year>1960 & .$year<=1970],na.rm=TRUE),
NO.CATCHES = any(.$rel.cat==0,na.rm=TRUE),
YEARS.ZERO.CATCHES = sum(.$rel.cat==0,na.rm=TRUE),
LITTLE.CATCHES = any(.$rel.cat<0.15,na.rm=TRUE),
LOG.MAX.CATCH = log(max(ifelse(is.na(.$catch.units),0.4*.$biomass,ifelse(.$catch.units=="SHORT TONS",.$catches*1000/1102.31,.$catches)),na.rm=TRUE)+0.001)
# AVG.CAT.LOW = mean(.$rel.cat[.$rel.bio<(0.3)],na.rm=TRUE),
# AVG.REC.LOW = mean(.$rel.rec[.$rel.bio<(0.3)],na.rm=TRUE),
# AVG.PRO.LOW = mean(.$productivity[.$rel.bio<(0.3)],na.rm=TRUE),
# MEAN.ABOVE.LOW = mean(.$rel.bio[.$rel.bio>=(0.3)]),
# AVG.CAT.ABOVE.LOW = mean(.$rel.cat[.$rel.bio>=(0.3)],na.rm=TRUE),
# AVG.REC.ABOVE.LOW = mean(.$rel.rec[.$rel.bio>=(0.3)],na.rm=TRUE),
# AVG.PRO.ABOVE.LOW = mean(.$productivity[.$rel.bio>=(0.3)],na.rm=TRUE),
# YEARS.VERY.LOW = sum(.$rel.bio<(0.15)),
# FRAC.VERY.LOW=sum(.$rel.bio<(0.15))/length(.$rel.bio),
# MEAN.VERY.LOW = mean(.$rel.bio[.$rel.bio<(0.15)],na.rm=TRUE),
# AVG.CAT.VERY.LOW = mean(.$rel.cat[.$rel.bio<(0.15)],na.rm=TRUE),
# AVG.REC.VERY.LOW = mean(.$rel.rec[.$rel.bio<(0.15)],na.rm=TRUE),
# AVG.PRO.VERY.LOW = mean(.$productivity[.$rel.bio<(0.15)],na.rm=TRUE),
# MEAN.ABOVE.VERY.LOW = mean(.$rel.bio[.$rel.bio>=(0.15)],na.rm=TRUE),
# AVG.CAT.ABOVE.VERY.LOW = mean(.$rel.cat[.$rel.bio>=(0.15)],na.rm=TRUE),
# AVG.REC.ABOVE.VERY.LOW = mean(.$rel.rec[.$rel.bio>=(0.15)],na.rm=TRUE),
# AVG.PRO.ABOVE.VERY.LOW = mean(.$productivity[.$rel.bio>=(0.15)],na.rm=TRUE),
# This calculation finds separate events, not those of the same trend
#NUM.0.7.declines=sum(.$crit.value>=0.7 & # If current decline is greater than....
#                       (lag(.$crit.value<0.7,1) | is.na(lag(.$crit.value<0.7,1))),na.rm=TRUE),
#NUM.0.5.declines=sum(.$crit.value>=0.5 & # If current decline is greater than....
#                       (lag(.$crit.value<0.5,1) | is.na(lag(.$crit.value<0.5,1))),na.rm=TRUE),
#NUM.0.3.declines=sum(.$crit.value>=0.3 & # If current decline is greater than....
#                       (lag(.$crit.value<0.3,1) | is.na(lag(.$crit.value<0.3,1))),na.rm=TRUE),
#MAX.DECLINE = .$crit.value[.$sequence==0],
#RATE.OF.DECLINE = .$crit.value[.$sequence==0]/unique(.$sequence.minimum),
#MIN.AFTER.DECLINE=min(.$rel.bio[.$sequence>=0 & .$sequence<=10],na.rm=TRUE),
#MIN.FROM.MAX=which(.$rel.bio==max(.$rel.bio,na.rm=TRUE))[1]-which(.$rel.bio==min(.$rel.bio,na.rm=TRUE))[1],
))
return(her_dat_stat)
}
# summary.table <- narrowed.down %>% group_by(StockID) %>%
#   summarise(Stock.group=unique(mainname),
#             Stock.name=unique(secondinfo),
#             LME=unique(LME),
#             Species=unique(species),
#             Data.type=unique(biomass.type),
#             Units.ssb=unique(biomass.units),
#             Units.rec=unique(recruit.units),
#             Units.cat=unique(catch.units),
#             Long=unique(longitude),
#             Lat=unique(latitude),
#             Age.at.maturity=unique(age.at.maturity),
#             Age.at.recruitment=unique(recruitment.age),
#             Mean.weight.age.5=unique(mean.weight.age.5),
#             Peak.spawning.month=unique(month.of.spawning))
# write.table(summary.table,"~/PWS_herring/Thesis/Meta_analysis/data/my_data/herring_locations.csv",sep=",",row.names=FALSE)
# ===========================================================================
# COLLAPSE, CV, AUTOCORRELATION, SPECTRAL, and HURST EXPONENT CHARACTERISTICS FOR BIOMASSS
# ===========================================================================
# EXPLORATORY - How many survey & how many stock assessment series?
series.type <- narrowed.down %>%
filter(!(secondinfo %in% c('Yellow Sea','Area 27 (North Vancouver Island)'))) %>%
group_by(mainname,secondinfo,species) %>% summarise(biomass.type=unique(biomass.type))
number.models <- sum(series.type$biomass.type=="MODEL",na.rm=TRUE)
number.surveys <- sum(series.type$biomass.type=="SURVEY",na.rm=TRUE)
sum(!is.na(series.type$biomass.type))
raw.data.summary <- narrowed.down%>%
filter(!(secondinfo %in% c('Yellow Sea','Area 27 (North Vancouver Island)')) & !is.na(biomass)) %>%
group_by(StockID,species,mainname,secondinfo,biomass.type,estimate.method,recruit.type) %>%
#filter(length(rel.bio)>=10) %>%
do(data.frame(
NO.SINGLE.YEARS.MISSING=sum(diff(.$year)==2),
TIMES.MULTIPLE.YEARS.MISSING=sum(diff(.$year)>2),
TS.LENGTH = length(.$biomass)))
sum(raw.data.summary$NO.SINGLE.YEARS.MISSING>0)
raw.data.summary$secondinfo[raw.data.summary$NO.SINGLE.YEARS.MISSING>0]
raw.data.summary$secondinfo[raw.data.summary$TIMES.MULTIPLE.YEARS.MISSING>0]
her_dat_filtered <- function_process_data(narrowed.down,filter_survey=TRUE)
her_dat_orig <- function_process_data(narrowed.down,filter_survey=FALSE)
lag.step <- 1
method <- "spearman"
pws <- her_dat_filtered %>% filter(secondinfo=="Prince William Sound") %>% mutate(rec.ratios=recruits/lag(recruits,na.rm=TRUE))
nor.spr.spa <- her_dat_filtered %>% filter(secondinfo=="Norwegian spring spawners") %>% summarize(HURST = function_calc_hurst(rel.bio))
ssb.var.filtered <- her_dat_filtered %>%
filter(!(secondinfo %in% c('Yellow Sea','Area 27 (North Vancouver Island)')) & !is.na(rel.bio)) %>%
group_by(StockID,species,mainname,secondinfo,biomass.type,estimate.method,recruit.type) %>%
#filter(length(rel.bio)>=10) %>%
do(data.frame(
NO.SINGLE.YEARS.MISSING=sum(diff(.$year)==2),
TIMES.MULTIPLE.YEARS.MISSING=sum(diff(.$year)>2),
TS.LENGTH = length(.$rel.bio),
CV = sd(.$rel.bio,na.rm=TRUE)/mean(.$rel.bio,na.rm=TRUE),
AC = acf(x=.$rel.bio,lag.max=lag.step,plot=FALSE)$acf[lag.step+1,1,1],
#HURST = hurstexp(.$rel.bio,display=FALSE)$Hs, # 0.5-1 is persistence, 0.5 is uncorrelated/random walk
HURST = function_calc_hurst(.$rel.bio), # 0.5-1 is persistence, 0.5 is uncorrelated/random walk
SPEC = ifelse(length(.$rel.bio)<8,NA,function_calc_beta(.$rel.bio)[1]))) # 0 is white noise, near 1 is red, near 2 is intense brown
ssb.var.raw <- her_dat_orig %>%
filter(!(secondinfo %in% c('Yellow Sea','Area 27 (North Vancouver Island)')) & !is.na(rel.bio)) %>%
group_by(StockID,species,mainname,secondinfo,biomass.type,estimate.method,recruit.type) %>%
# filter(length(rel.bio)>=10) %>%
do(data.frame(
NO.SINGLE.YEARS.MISSING=sum(diff(.$year)==2),
TIMES.MULTIPLE.YEARS.MISSING=sum(diff(.$year)>2),
TS.LENGTH = length(.$rel.bio),
CV = sd(.$rel.bio,na.rm=TRUE)/mean(.$rel.bio,na.rm=TRUE),
AC = acf(x=.$rel.bio,lag.max=lag.step,plot=FALSE)$acf[lag.step+1,1,1],
HURST = function_calc_hurst(.$rel.bio), # 0.5-1 is persistence, 0.5 is uncorrelated/random walk
SPEC = ifelse(length(.$rel.bio)<8,NA,function_calc_beta(.$rel.bio)[1]))) # 0 is white noise, near 1 is red, near 2 is intense brown
her_dat <- her_dat_filtered
all.herring.years <- dat_stat(her_dat)
